{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "681db7de-56df-408d-8dd9-cba3cc2ded66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2044661c-e7ea-4db6-ab45-8afc383f1330",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9ebbcfb-9d28-4019-b725-28fe7875e782",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SAVEE = './dataset/ALL/'\n",
    "dir_list = os.listdir(SAVEE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e809d054-2ac3-4243-99de-88f09da2342c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "emotion=[]\n",
    "path = []\n",
    "for i in dir_list:\n",
    "    if i[-8:-6]=='_a':\n",
    "        emotion.append('angry')\n",
    "    elif i[-8:-6]=='_d':\n",
    "        emotion.append('disgust')\n",
    "    elif i[-8:-6]=='_f':\n",
    "        emotion.append('fear')\n",
    "    elif i[-8:-6]=='_h':\n",
    "        emotion.append('happy')\n",
    "    elif i[-8:-6]=='_n':\n",
    "        emotion.append('neutral')\n",
    "    elif i[-8:-6]=='sa':\n",
    "        emotion.append('sad')\n",
    "    elif i[-8:-6]=='su':\n",
    "        emotion.append('surprise')\n",
    "    else:\n",
    "        emotion.append('unknown') \n",
    "    path.append(SAVEE + i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "980cde5f-fc9c-489e-9293-fe20d0ab1a19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVEE dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>angry</td>\n",
       "      <td>./dataset/ALL/DC_a01.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>angry</td>\n",
       "      <td>./dataset/ALL/DC_a02.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>angry</td>\n",
       "      <td>./dataset/ALL/DC_a03.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>angry</td>\n",
       "      <td>./dataset/ALL/DC_a04.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>angry</td>\n",
       "      <td>./dataset/ALL/DC_a05.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  labels                      path\n",
       "0  angry  ./dataset/ALL/DC_a01.wav\n",
       "1  angry  ./dataset/ALL/DC_a02.wav\n",
       "2  angry  ./dataset/ALL/DC_a03.wav\n",
       "3  angry  ./dataset/ALL/DC_a04.wav\n",
       "4  angry  ./dataset/ALL/DC_a05.wav"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAVEE_df = pd.DataFrame(emotion, columns = ['labels'])\n",
    "SAVEE_df = pd.concat([SAVEE_df, pd.DataFrame(path, columns = ['path'])], axis = 1)\n",
    "print('SAVEE dataset')\n",
    "SAVEE_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1de31143-1520-402c-9db3-8c15534ef058",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TESS = './dataset/TESS Toronto emotional speech set data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bee0fbfd-2302-4177-98ca-751f7d371fcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = []\n",
    "emotion = []\n",
    "dir_list = os.listdir(TESS)\n",
    "\n",
    "for i in dir_list:\n",
    "    fname = os.listdir(TESS + i)   \n",
    "    for f in fname:\n",
    "        if i == 'OAF_angry' or i == 'YAF_angry':\n",
    "            emotion.append('angry')\n",
    "        elif i == 'OAF_disgust' or i == 'YAF_disgust':\n",
    "            emotion.append('disgust')\n",
    "        elif i == 'OAF_Fear' or i == 'YAF_fear':\n",
    "            emotion.append('fear')\n",
    "        elif i == 'OAF_happy' or i == 'YAF_happy':\n",
    "            emotion.append('happy')\n",
    "        elif i == 'OAF_neutral' or i == 'YAF_neutral':\n",
    "            emotion.append('neutral')                                \n",
    "        elif i == 'OAF_Pleasant_surprise' or i == 'YAF_pleasant_surprised':\n",
    "            emotion.append('surprise')               \n",
    "        elif i == 'OAF_Sad' or i == 'YAF_sad':\n",
    "            emotion.append('sad')\n",
    "        else:\n",
    "            emotion.append('Unknown')\n",
    "        path.append(TESS + i + \"/\" + f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fce0112-5797-4bb1-90be-65f2ca7e8098",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESS dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>angry</td>\n",
       "      <td>./dataset/TESS Toronto emotional speech set da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>angry</td>\n",
       "      <td>./dataset/TESS Toronto emotional speech set da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>angry</td>\n",
       "      <td>./dataset/TESS Toronto emotional speech set da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>angry</td>\n",
       "      <td>./dataset/TESS Toronto emotional speech set da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>angry</td>\n",
       "      <td>./dataset/TESS Toronto emotional speech set da...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  labels                                               path\n",
       "0  angry  ./dataset/TESS Toronto emotional speech set da...\n",
       "1  angry  ./dataset/TESS Toronto emotional speech set da...\n",
       "2  angry  ./dataset/TESS Toronto emotional speech set da...\n",
       "3  angry  ./dataset/TESS Toronto emotional speech set da...\n",
       "4  angry  ./dataset/TESS Toronto emotional speech set da..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TESS_df = pd.DataFrame(emotion, columns = ['labels'])\n",
    "#TESS_df['source'] = 'TESS'\n",
    "TESS_df = pd.concat([TESS_df,pd.DataFrame(path, columns = ['path'])],axis=1)\n",
    "print('TESS dataset')\n",
    "TESS_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "314f9cd3-557c-4c97-8942-cbbc7f685c0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now lets merge all the dataframe\n",
    "Males = pd.concat([SAVEE_df], axis = 0)\n",
    "Males.to_csv(\"males_emotions_df.csv\", index = False)\n",
    "\n",
    "Females = pd.concat([TESS_df], axis = 0)\n",
    "Females.to_csv(\"females_emotions_df.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "233a50ec-acbd-4b99-881e-a7eaf40f8119",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "comb = [Males,Females]\n",
    "Dataset=pd.concat(comb)\n",
    "Dataset=Dataset.sort_values(by=['labels'])\n",
    "Dataset.to_csv(\"Dataset_df.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe24779-08f0-49fa-8f40-79f76647b68a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "224c4b78-eea9-40af-a1e2-d7951c40fdcf",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6f0ffe7-6225-458b-b0c4-1b1e9b0e3921",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.utils import to_categorical \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder=LabelEncoder()\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49186158-f690-4a3a-af92-0ac91b95c2cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_extractor(file):\n",
    "    data,sample_rate=librosa.load(file,res_type=\"kaiser_fast\")\n",
    "    \n",
    "    #MFCC \n",
    "    mfccs_features = librosa.feature.mfcc(y=data,sr=sample_rate,n_mfcc=40)\n",
    "    mfccs_scaled_features = np.mean(mfccs_features.T,axis=0)\n",
    "\n",
    "    return mfccs_scaled_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90a9997a-1999-498b-8512-4b91692ad926",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3280it [01:26, 37.97it/s]\n"
     ]
    }
   ],
   "source": [
    "extracted_features=[]\n",
    "for index_num,row in tqdm(Dataset.iterrows()):\n",
    "    file_name = os.path.join(str(row[\"path\"]))\n",
    "    final_class_labels=row['labels']\n",
    "    data=feature_extractor(file_name)\n",
    "    extracted_features.append([data,final_class_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35c521d4-a230-43c9-a8e2-19e81be11d6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.18135529e+02  1.12588829e+02  1.23745937e+01  3.45954971e+01\n",
      "  1.16200695e+01 -2.91651773e+00 -2.48573990e+01 -2.59529781e+00\n",
      " -1.00110602e+00 -1.15826321e+01 -2.48748541e+00 -9.69559789e-01\n",
      " -9.28020418e-01 -1.63937509e+00  5.17986345e+00  6.59545600e-01\n",
      " -3.40372515e+00  5.59151697e+00 -9.27957833e-01 -5.73660564e+00\n",
      "  5.74505851e-02  5.40333331e-01  1.78466511e+00 -2.08028388e+00\n",
      " -1.25319338e+00 -3.53276229e+00 -4.19973946e+00 -1.58740610e-01\n",
      " -2.00895476e+00  5.86389303e-01  1.40789413e+00  1.71946561e+00\n",
      "  3.23082638e+00  2.73758245e+00  3.46699786e+00  5.05381870e+00\n",
      "  4.83345985e+00  5.60282660e+00  4.30647087e+00  3.39099669e+00]\n"
     ]
    }
   ],
   "source": [
    "extracted_features_df = pd.DataFrame(extracted_features,columns=['feature','class'])\n",
    "extracted_features_df.head()\n",
    "print(extracted_features_df.iloc[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47d7567d-2f02-47d6-865a-54c140c6c381",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#split in indep and dep dataset\n",
    "X = np.array(extracted_features_df['feature'].tolist())\n",
    "y = np.array(extracted_features_df['class'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a012ffac-b3bf-4d78-845b-d72c45dabe68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y=to_categorical(labelencoder.fit_transform(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b2db96e-d88f-46a3-a884-e4d6ff7ab6eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dc8eee-1846-4324-aa4e-147a18a65178",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a203bc12-4341-43f6-a524-4cf2164233d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,Activation,Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4eba89cf-2f7a-41f0-b8f7-b8a7f5a05c8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_labels = y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "372e780f-7a0e-414f-9e8f-a31583e029fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "#first layer\n",
    "model.add(Dense(100,input_shape=(40,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "#second layer\n",
    "model.add(Dense(200))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "#third layer\n",
    "model.add(Dense(100))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#final layer\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39de56ef-d1a0-4ebd-ad41-bf8ae7bbcb25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 100)               4100      \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 200)               20200     \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 200)               0         \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 200)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 100)               20100     \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 7)                 707       \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 7)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 45,107\n",
      "Trainable params: 45,107\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e4de4c3-af87-44fb-8c5d-e891079333b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dba8632-4167-4a39-ad6c-ad14c4d01e78",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "237775e8-52ac-45ec-89aa-7a89b5e1a531",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "62/82 [=====================>........] - ETA: 0s - loss: 31.6055 - accuracy: 0.1547\n",
      "Epoch 1: val_loss improved from inf to 1.99515, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 1s 6ms/step - loss: 26.6842 - accuracy: 0.1566 - val_loss: 1.9952 - val_accuracy: 0.1067\n",
      "Epoch 2/100\n",
      "61/82 [=====================>........] - ETA: 0s - loss: 6.6217 - accuracy: 0.1588\n",
      "Epoch 2: val_loss improved from 1.99515 to 1.94748, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 6.0031 - accuracy: 0.1551 - val_loss: 1.9475 - val_accuracy: 0.1204\n",
      "Epoch 3/100\n",
      "58/82 [====================>.........] - ETA: 0s - loss: 3.1839 - accuracy: 0.1595\n",
      "Epoch 3: val_loss did not improve from 1.94748\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 3.0142 - accuracy: 0.1521 - val_loss: 1.9481 - val_accuracy: 0.1296\n",
      "Epoch 4/100\n",
      "58/82 [====================>.........] - ETA: 0s - loss: 2.4055 - accuracy: 0.1622\n",
      "Epoch 4: val_loss did not improve from 1.94748\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 2.3682 - accuracy: 0.1585 - val_loss: 1.9484 - val_accuracy: 0.1204\n",
      "Epoch 5/100\n",
      "59/82 [====================>.........] - ETA: 0s - loss: 2.1788 - accuracy: 0.1536\n",
      "Epoch 5: val_loss did not improve from 1.94748\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 2.1811 - accuracy: 0.1524 - val_loss: 1.9488 - val_accuracy: 0.1204\n",
      "Epoch 6/100\n",
      "58/82 [====================>.........] - ETA: 0s - loss: 2.1085 - accuracy: 0.1595\n",
      "Epoch 6: val_loss did not improve from 1.94748\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 2.0887 - accuracy: 0.1585 - val_loss: 1.9489 - val_accuracy: 0.1204\n",
      "Epoch 7/100\n",
      "78/82 [===========================>..] - ETA: 0s - loss: 2.0103 - accuracy: 0.1607\n",
      "Epoch 7: val_loss did not improve from 1.94748\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 2.0095 - accuracy: 0.1631 - val_loss: 1.9490 - val_accuracy: 0.1204\n",
      "Epoch 8/100\n",
      "54/82 [==================>...........] - ETA: 0s - loss: 2.0005 - accuracy: 0.1487\n",
      "Epoch 8: val_loss did not improve from 1.94748\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 2.0021 - accuracy: 0.1555 - val_loss: 1.9491 - val_accuracy: 0.1204\n",
      "Epoch 9/100\n",
      "54/82 [==================>...........] - ETA: 0s - loss: 1.9619 - accuracy: 0.1875\n",
      "Epoch 9: val_loss improved from 1.94748 to 1.94034, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 1.9615 - accuracy: 0.1837 - val_loss: 1.9403 - val_accuracy: 0.1585\n",
      "Epoch 10/100\n",
      "57/82 [===================>..........] - ETA: 0s - loss: 1.9431 - accuracy: 0.1941\n",
      "Epoch 10: val_loss improved from 1.94034 to 1.92262, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 1.9378 - accuracy: 0.1902 - val_loss: 1.9226 - val_accuracy: 0.1936\n",
      "Epoch 11/100\n",
      "81/82 [============================>.] - ETA: 0s - loss: 1.9507 - accuracy: 0.1952\n",
      "Epoch 11: val_loss improved from 1.92262 to 1.84691, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 1.9517 - accuracy: 0.1959 - val_loss: 1.8469 - val_accuracy: 0.2180\n",
      "Epoch 12/100\n",
      "59/82 [====================>.........] - ETA: 0s - loss: 1.8765 - accuracy: 0.2055\n",
      "Epoch 12: val_loss improved from 1.84691 to 1.77576, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 1.8738 - accuracy: 0.2134 - val_loss: 1.7758 - val_accuracy: 0.2409\n",
      "Epoch 13/100\n",
      "59/82 [====================>.........] - ETA: 0s - loss: 1.8667 - accuracy: 0.2214\n",
      "Epoch 13: val_loss did not improve from 1.77576\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 1.8550 - accuracy: 0.2264 - val_loss: 1.7765 - val_accuracy: 0.2393\n",
      "Epoch 14/100\n",
      "56/82 [===================>..........] - ETA: 0s - loss: 1.8178 - accuracy: 0.2316\n",
      "Epoch 14: val_loss improved from 1.77576 to 1.76301, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 1.8239 - accuracy: 0.2317 - val_loss: 1.7630 - val_accuracy: 0.2241\n",
      "Epoch 15/100\n",
      "58/82 [====================>.........] - ETA: 0s - loss: 1.7754 - accuracy: 0.2511\n",
      "Epoch 15: val_loss improved from 1.76301 to 1.71071, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 1.7797 - accuracy: 0.2477 - val_loss: 1.7107 - val_accuracy: 0.2271\n",
      "Epoch 16/100\n",
      "59/82 [====================>.........] - ETA: 0s - loss: 1.7339 - accuracy: 0.2638\n",
      "Epoch 16: val_loss improved from 1.71071 to 1.59647, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 1.7294 - accuracy: 0.2626 - val_loss: 1.5965 - val_accuracy: 0.3674\n",
      "Epoch 17/100\n",
      "61/82 [=====================>........] - ETA: 0s - loss: 1.6465 - accuracy: 0.2792\n",
      "Epoch 17: val_loss improved from 1.59647 to 1.44090, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 1.6436 - accuracy: 0.2858 - val_loss: 1.4409 - val_accuracy: 0.3247\n",
      "Epoch 18/100\n",
      "61/82 [=====================>........] - ETA: 0s - loss: 1.6144 - accuracy: 0.3038\n",
      "Epoch 18: val_loss improved from 1.44090 to 1.42461, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 1.6172 - accuracy: 0.3075 - val_loss: 1.4246 - val_accuracy: 0.4527\n",
      "Epoch 19/100\n",
      "55/82 [===================>..........] - ETA: 0s - loss: 1.5579 - accuracy: 0.3449\n",
      "Epoch 19: val_loss improved from 1.42461 to 1.37132, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 1.5636 - accuracy: 0.3430 - val_loss: 1.3713 - val_accuracy: 0.5259\n",
      "Epoch 20/100\n",
      "57/82 [===================>..........] - ETA: 0s - loss: 1.5327 - accuracy: 0.3602\n",
      "Epoch 20: val_loss improved from 1.37132 to 1.34008, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 1.5249 - accuracy: 0.3617 - val_loss: 1.3401 - val_accuracy: 0.4802\n",
      "Epoch 21/100\n",
      "80/82 [============================>.] - ETA: 0s - loss: 1.4642 - accuracy: 0.3875\n",
      "Epoch 21: val_loss improved from 1.34008 to 1.26218, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 1.4623 - accuracy: 0.3899 - val_loss: 1.2622 - val_accuracy: 0.5427\n",
      "Epoch 22/100\n",
      "73/82 [=========================>....] - ETA: 0s - loss: 1.4271 - accuracy: 0.4170\n",
      "Epoch 22: val_loss improved from 1.26218 to 1.23080, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 1.4231 - accuracy: 0.4177 - val_loss: 1.2308 - val_accuracy: 0.6479\n",
      "Epoch 23/100\n",
      "60/82 [====================>.........] - ETA: 0s - loss: 1.3681 - accuracy: 0.4385\n",
      "Epoch 23: val_loss improved from 1.23080 to 1.16907, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 1.3563 - accuracy: 0.4428 - val_loss: 1.1691 - val_accuracy: 0.6966\n",
      "Epoch 24/100\n",
      "56/82 [===================>..........] - ETA: 0s - loss: 1.3287 - accuracy: 0.4626\n",
      "Epoch 24: val_loss improved from 1.16907 to 1.08022, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 1.3128 - accuracy: 0.4619 - val_loss: 1.0802 - val_accuracy: 0.7256\n",
      "Epoch 25/100\n",
      "58/82 [====================>.........] - ETA: 0s - loss: 1.2680 - accuracy: 0.4978\n",
      "Epoch 25: val_loss improved from 1.08022 to 1.02238, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 1.2570 - accuracy: 0.5050 - val_loss: 1.0224 - val_accuracy: 0.6966\n",
      "Epoch 26/100\n",
      "55/82 [===================>..........] - ETA: 0s - loss: 1.2187 - accuracy: 0.5324\n",
      "Epoch 26: val_loss improved from 1.02238 to 0.97214, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 1.2101 - accuracy: 0.5373 - val_loss: 0.9721 - val_accuracy: 0.7515\n",
      "Epoch 27/100\n",
      "63/82 [======================>.......] - ETA: 0s - loss: 1.1710 - accuracy: 0.5516\n",
      "Epoch 27: val_loss improved from 0.97214 to 0.88472, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 1.1509 - accuracy: 0.5575 - val_loss: 0.8847 - val_accuracy: 0.7485\n",
      "Epoch 28/100\n",
      "57/82 [===================>..........] - ETA: 0s - loss: 1.1193 - accuracy: 0.5833\n",
      "Epoch 28: val_loss improved from 0.88472 to 0.87026, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 1.0900 - accuracy: 0.5941 - val_loss: 0.8703 - val_accuracy: 0.7729\n",
      "Epoch 29/100\n",
      "55/82 [===================>..........] - ETA: 0s - loss: 1.0539 - accuracy: 0.6114\n",
      "Epoch 29: val_loss improved from 0.87026 to 0.78060, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 1.0305 - accuracy: 0.6159 - val_loss: 0.7806 - val_accuracy: 0.7530\n",
      "Epoch 30/100\n",
      "63/82 [======================>.......] - ETA: 0s - loss: 0.9950 - accuracy: 0.6310\n",
      "Epoch 30: val_loss improved from 0.78060 to 0.72508, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.9850 - accuracy: 0.6307 - val_loss: 0.7251 - val_accuracy: 0.7896\n",
      "Epoch 31/100\n",
      "73/82 [=========================>....] - ETA: 0s - loss: 0.9423 - accuracy: 0.6592\n",
      "Epoch 31: val_loss improved from 0.72508 to 0.68274, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.9407 - accuracy: 0.6627 - val_loss: 0.6827 - val_accuracy: 0.7973\n",
      "Epoch 32/100\n",
      "62/82 [=====================>........] - ETA: 0s - loss: 0.8645 - accuracy: 0.6789\n",
      "Epoch 32: val_loss improved from 0.68274 to 0.61951, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.8668 - accuracy: 0.6803 - val_loss: 0.6195 - val_accuracy: 0.7896\n",
      "Epoch 33/100\n",
      "60/82 [====================>.........] - ETA: 0s - loss: 0.8444 - accuracy: 0.6922\n",
      "Epoch 33: val_loss improved from 0.61951 to 0.60203, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.8400 - accuracy: 0.6947 - val_loss: 0.6020 - val_accuracy: 0.8308\n",
      "Epoch 34/100\n",
      "61/82 [=====================>........] - ETA: 0s - loss: 0.8283 - accuracy: 0.7147\n",
      "Epoch 34: val_loss improved from 0.60203 to 0.57065, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.8240 - accuracy: 0.7111 - val_loss: 0.5707 - val_accuracy: 0.8415\n",
      "Epoch 35/100\n",
      "62/82 [=====================>........] - ETA: 0s - loss: 0.7717 - accuracy: 0.7324\n",
      "Epoch 35: val_loss improved from 0.57065 to 0.55731, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.7685 - accuracy: 0.7279 - val_loss: 0.5573 - val_accuracy: 0.8338\n",
      "Epoch 36/100\n",
      "58/82 [====================>.........] - ETA: 0s - loss: 0.7609 - accuracy: 0.7344\n",
      "Epoch 36: val_loss improved from 0.55731 to 0.54857, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.7587 - accuracy: 0.7287 - val_loss: 0.5486 - val_accuracy: 0.8399\n",
      "Epoch 37/100\n",
      "60/82 [====================>.........] - ETA: 0s - loss: 0.7313 - accuracy: 0.7318\n",
      "Epoch 37: val_loss improved from 0.54857 to 0.51708, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.7253 - accuracy: 0.7374 - val_loss: 0.5171 - val_accuracy: 0.8445\n",
      "Epoch 38/100\n",
      "59/82 [====================>.........] - ETA: 0s - loss: 0.7084 - accuracy: 0.7415\n",
      "Epoch 38: val_loss did not improve from 0.51708\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.7158 - accuracy: 0.7405 - val_loss: 0.5314 - val_accuracy: 0.8415\n",
      "Epoch 39/100\n",
      "56/82 [===================>..........] - ETA: 0s - loss: 0.6804 - accuracy: 0.7567\n",
      "Epoch 39: val_loss improved from 0.51708 to 0.48104, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.6970 - accuracy: 0.7477 - val_loss: 0.4810 - val_accuracy: 0.8338\n",
      "Epoch 40/100\n",
      "59/82 [====================>.........] - ETA: 0s - loss: 0.6524 - accuracy: 0.7728\n",
      "Epoch 40: val_loss improved from 0.48104 to 0.46403, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.6622 - accuracy: 0.7687 - val_loss: 0.4640 - val_accuracy: 0.8460\n",
      "Epoch 41/100\n",
      "63/82 [======================>.......] - ETA: 0s - loss: 0.6668 - accuracy: 0.7738\n",
      "Epoch 41: val_loss improved from 0.46403 to 0.45283, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.6605 - accuracy: 0.7755 - val_loss: 0.4528 - val_accuracy: 0.8460\n",
      "Epoch 42/100\n",
      "61/82 [=====================>........] - ETA: 0s - loss: 0.6301 - accuracy: 0.7823\n",
      "Epoch 42: val_loss improved from 0.45283 to 0.41373, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.6255 - accuracy: 0.7774 - val_loss: 0.4137 - val_accuracy: 0.8445\n",
      "Epoch 43/100\n",
      "54/82 [==================>...........] - ETA: 0s - loss: 0.6029 - accuracy: 0.7720\n",
      "Epoch 43: val_loss did not improve from 0.41373\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.5997 - accuracy: 0.7805 - val_loss: 0.4360 - val_accuracy: 0.8476\n",
      "Epoch 44/100\n",
      "55/82 [===================>..........] - ETA: 0s - loss: 0.5906 - accuracy: 0.8006\n",
      "Epoch 44: val_loss improved from 0.41373 to 0.40336, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.5894 - accuracy: 0.7961 - val_loss: 0.4034 - val_accuracy: 0.8445\n",
      "Epoch 45/100\n",
      "62/82 [=====================>........] - ETA: 0s - loss: 0.5641 - accuracy: 0.7994\n",
      "Epoch 45: val_loss improved from 0.40336 to 0.36967, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.5480 - accuracy: 0.8037 - val_loss: 0.3697 - val_accuracy: 0.8537\n",
      "Epoch 46/100\n",
      "70/82 [========================>.....] - ETA: 0s - loss: 0.5520 - accuracy: 0.8022\n",
      "Epoch 46: val_loss improved from 0.36967 to 0.36678, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.5385 - accuracy: 0.8095 - val_loss: 0.3668 - val_accuracy: 0.8598\n",
      "Epoch 47/100\n",
      "55/82 [===================>..........] - ETA: 0s - loss: 0.5245 - accuracy: 0.8250\n",
      "Epoch 47: val_loss did not improve from 0.36678\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.5417 - accuracy: 0.8190 - val_loss: 0.3778 - val_accuracy: 0.8674\n",
      "Epoch 48/100\n",
      "55/82 [===================>..........] - ETA: 0s - loss: 0.4881 - accuracy: 0.8295\n",
      "Epoch 48: val_loss improved from 0.36678 to 0.35145, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.5109 - accuracy: 0.8213 - val_loss: 0.3514 - val_accuracy: 0.8628\n",
      "Epoch 49/100\n",
      "57/82 [===================>..........] - ETA: 0s - loss: 0.5334 - accuracy: 0.8152\n",
      "Epoch 49: val_loss improved from 0.35145 to 0.33652, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.5291 - accuracy: 0.8175 - val_loss: 0.3365 - val_accuracy: 0.8659\n",
      "Epoch 50/100\n",
      "56/82 [===================>..........] - ETA: 0s - loss: 0.5202 - accuracy: 0.8192\n",
      "Epoch 50: val_loss improved from 0.33652 to 0.29336, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.4941 - accuracy: 0.8270 - val_loss: 0.2934 - val_accuracy: 0.8780\n",
      "Epoch 51/100\n",
      "53/82 [==================>...........] - ETA: 0s - loss: 0.4878 - accuracy: 0.8320\n",
      "Epoch 51: val_loss did not improve from 0.29336\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.4808 - accuracy: 0.8289 - val_loss: 0.3063 - val_accuracy: 0.8765\n",
      "Epoch 52/100\n",
      "58/82 [====================>.........] - ETA: 0s - loss: 0.4916 - accuracy: 0.8227\n",
      "Epoch 52: val_loss improved from 0.29336 to 0.28158, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.4768 - accuracy: 0.8308 - val_loss: 0.2816 - val_accuracy: 0.8902\n",
      "Epoch 53/100\n",
      "57/82 [===================>..........] - ETA: 0s - loss: 0.4700 - accuracy: 0.8268\n",
      "Epoch 53: val_loss improved from 0.28158 to 0.28065, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.4700 - accuracy: 0.8281 - val_loss: 0.2806 - val_accuracy: 0.8887\n",
      "Epoch 54/100\n",
      "59/82 [====================>.........] - ETA: 0s - loss: 0.4697 - accuracy: 0.8263\n",
      "Epoch 54: val_loss improved from 0.28065 to 0.27910, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.4498 - accuracy: 0.8361 - val_loss: 0.2791 - val_accuracy: 0.8933\n",
      "Epoch 55/100\n",
      "58/82 [====================>.........] - ETA: 0s - loss: 0.4385 - accuracy: 0.8427\n",
      "Epoch 55: val_loss improved from 0.27910 to 0.27110, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.4309 - accuracy: 0.8476 - val_loss: 0.2711 - val_accuracy: 0.8933\n",
      "Epoch 56/100\n",
      "70/82 [========================>.....] - ETA: 0s - loss: 0.4465 - accuracy: 0.8353\n",
      "Epoch 56: val_loss did not improve from 0.27110\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.4475 - accuracy: 0.8380 - val_loss: 0.2726 - val_accuracy: 0.8979\n",
      "Epoch 57/100\n",
      "78/82 [===========================>..] - ETA: 0s - loss: 0.4531 - accuracy: 0.8409\n",
      "Epoch 57: val_loss improved from 0.27110 to 0.25838, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 4ms/step - loss: 0.4621 - accuracy: 0.8338 - val_loss: 0.2584 - val_accuracy: 0.8994\n",
      "Epoch 58/100\n",
      "67/82 [=======================>......] - ETA: 0s - loss: 0.4563 - accuracy: 0.8433\n",
      "Epoch 58: val_loss did not improve from 0.25838\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.4559 - accuracy: 0.8399 - val_loss: 0.2906 - val_accuracy: 0.8735\n",
      "Epoch 59/100\n",
      "64/82 [======================>.......] - ETA: 0s - loss: 0.4433 - accuracy: 0.8433\n",
      "Epoch 59: val_loss did not improve from 0.25838\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.4501 - accuracy: 0.8373 - val_loss: 0.2675 - val_accuracy: 0.8887\n",
      "Epoch 60/100\n",
      "67/82 [=======================>......] - ETA: 0s - loss: 0.4347 - accuracy: 0.8545\n",
      "Epoch 60: val_loss improved from 0.25838 to 0.24978, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.4330 - accuracy: 0.8525 - val_loss: 0.2498 - val_accuracy: 0.9024\n",
      "Epoch 61/100\n",
      "72/82 [=========================>....] - ETA: 0s - loss: 0.4463 - accuracy: 0.8442\n",
      "Epoch 61: val_loss improved from 0.24978 to 0.24092, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.4386 - accuracy: 0.8441 - val_loss: 0.2409 - val_accuracy: 0.9070\n",
      "Epoch 62/100\n",
      "68/82 [=======================>......] - ETA: 0s - loss: 0.4212 - accuracy: 0.8387\n",
      "Epoch 62: val_loss did not improve from 0.24092\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.4346 - accuracy: 0.8369 - val_loss: 0.2431 - val_accuracy: 0.9085\n",
      "Epoch 63/100\n",
      "70/82 [========================>.....] - ETA: 0s - loss: 0.4159 - accuracy: 0.8469\n",
      "Epoch 63: val_loss did not improve from 0.24092\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.4087 - accuracy: 0.8506 - val_loss: 0.2571 - val_accuracy: 0.9024\n",
      "Epoch 64/100\n",
      "68/82 [=======================>......] - ETA: 0s - loss: 0.4058 - accuracy: 0.8562\n",
      "Epoch 64: val_loss did not improve from 0.24092\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.4058 - accuracy: 0.8552 - val_loss: 0.2528 - val_accuracy: 0.8979\n",
      "Epoch 65/100\n",
      "66/82 [=======================>......] - ETA: 0s - loss: 0.4196 - accuracy: 0.8452\n",
      "Epoch 65: val_loss did not improve from 0.24092\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.4013 - accuracy: 0.8537 - val_loss: 0.2473 - val_accuracy: 0.9085\n",
      "Epoch 66/100\n",
      "66/82 [=======================>......] - ETA: 0s - loss: 0.4120 - accuracy: 0.8608\n",
      "Epoch 66: val_loss did not improve from 0.24092\n",
      "82/82 [==============================] - 0s 4ms/step - loss: 0.4070 - accuracy: 0.8617 - val_loss: 0.2430 - val_accuracy: 0.9070\n",
      "Epoch 67/100\n",
      "68/82 [=======================>......] - ETA: 0s - loss: 0.4032 - accuracy: 0.8539\n",
      "Epoch 67: val_loss did not improve from 0.24092\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.3942 - accuracy: 0.8559 - val_loss: 0.2568 - val_accuracy: 0.9024\n",
      "Epoch 68/100\n",
      "67/82 [=======================>......] - ETA: 0s - loss: 0.4026 - accuracy: 0.8545\n",
      "Epoch 68: val_loss did not improve from 0.24092\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.4079 - accuracy: 0.8563 - val_loss: 0.2675 - val_accuracy: 0.8979\n",
      "Epoch 69/100\n",
      "72/82 [=========================>....] - ETA: 0s - loss: 0.3935 - accuracy: 0.8576\n",
      "Epoch 69: val_loss did not improve from 0.24092\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.3930 - accuracy: 0.8563 - val_loss: 0.2458 - val_accuracy: 0.9070\n",
      "Epoch 70/100\n",
      "80/82 [============================>.] - ETA: 0s - loss: 0.4083 - accuracy: 0.8488\n",
      "Epoch 70: val_loss did not improve from 0.24092\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.4121 - accuracy: 0.8464 - val_loss: 0.2588 - val_accuracy: 0.9070\n",
      "Epoch 71/100\n",
      "53/82 [==================>...........] - ETA: 0s - loss: 0.3812 - accuracy: 0.8573\n",
      "Epoch 71: val_loss improved from 0.24092 to 0.23668, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.3811 - accuracy: 0.8567 - val_loss: 0.2367 - val_accuracy: 0.9055\n",
      "Epoch 72/100\n",
      "55/82 [===================>..........] - ETA: 0s - loss: 0.3924 - accuracy: 0.8534\n",
      "Epoch 72: val_loss did not improve from 0.23668\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.3686 - accuracy: 0.8598 - val_loss: 0.2463 - val_accuracy: 0.9085\n",
      "Epoch 73/100\n",
      "57/82 [===================>..........] - ETA: 0s - loss: 0.3676 - accuracy: 0.8618\n",
      "Epoch 73: val_loss did not improve from 0.23668\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.3740 - accuracy: 0.8609 - val_loss: 0.2468 - val_accuracy: 0.9055\n",
      "Epoch 74/100\n",
      "57/82 [===================>..........] - ETA: 0s - loss: 0.3694 - accuracy: 0.8613\n",
      "Epoch 74: val_loss did not improve from 0.23668\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.3627 - accuracy: 0.8659 - val_loss: 0.2469 - val_accuracy: 0.8994\n",
      "Epoch 75/100\n",
      "56/82 [===================>..........] - ETA: 0s - loss: 0.3720 - accuracy: 0.8605\n",
      "Epoch 75: val_loss improved from 0.23668 to 0.22778, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.3645 - accuracy: 0.8639 - val_loss: 0.2278 - val_accuracy: 0.9101\n",
      "Epoch 76/100\n",
      "55/82 [===================>..........] - ETA: 0s - loss: 0.3763 - accuracy: 0.8625\n",
      "Epoch 76: val_loss did not improve from 0.22778\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.3707 - accuracy: 0.8670 - val_loss: 0.2390 - val_accuracy: 0.9055\n",
      "Epoch 77/100\n",
      "57/82 [===================>..........] - ETA: 0s - loss: 0.3803 - accuracy: 0.8586\n",
      "Epoch 77: val_loss did not improve from 0.22778\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.3806 - accuracy: 0.8567 - val_loss: 0.2316 - val_accuracy: 0.9085\n",
      "Epoch 78/100\n",
      "57/82 [===================>..........] - ETA: 0s - loss: 0.3665 - accuracy: 0.8673\n",
      "Epoch 78: val_loss improved from 0.22778 to 0.22319, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.3668 - accuracy: 0.8659 - val_loss: 0.2232 - val_accuracy: 0.9146\n",
      "Epoch 79/100\n",
      "59/82 [====================>.........] - ETA: 0s - loss: 0.3648 - accuracy: 0.8734\n",
      "Epoch 79: val_loss did not improve from 0.22319\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.3686 - accuracy: 0.8700 - val_loss: 0.2318 - val_accuracy: 0.9024\n",
      "Epoch 80/100\n",
      "56/82 [===================>..........] - ETA: 0s - loss: 0.3740 - accuracy: 0.8655\n",
      "Epoch 80: val_loss did not improve from 0.22319\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.3643 - accuracy: 0.8666 - val_loss: 0.2374 - val_accuracy: 0.9055\n",
      "Epoch 81/100\n",
      "57/82 [===================>..........] - ETA: 0s - loss: 0.3744 - accuracy: 0.8635\n",
      "Epoch 81: val_loss did not improve from 0.22319\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.3785 - accuracy: 0.8567 - val_loss: 0.2342 - val_accuracy: 0.9085\n",
      "Epoch 82/100\n",
      "56/82 [===================>..........] - ETA: 0s - loss: 0.3539 - accuracy: 0.8739\n",
      "Epoch 82: val_loss did not improve from 0.22319\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.3454 - accuracy: 0.8742 - val_loss: 0.2413 - val_accuracy: 0.9101\n",
      "Epoch 83/100\n",
      "57/82 [===================>..........] - ETA: 0s - loss: 0.3453 - accuracy: 0.8651\n",
      "Epoch 83: val_loss did not improve from 0.22319\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.3532 - accuracy: 0.8681 - val_loss: 0.2243 - val_accuracy: 0.9040\n",
      "Epoch 84/100\n",
      "57/82 [===================>..........] - ETA: 0s - loss: 0.3865 - accuracy: 0.8695\n",
      "Epoch 84: val_loss did not improve from 0.22319\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.3798 - accuracy: 0.8674 - val_loss: 0.2330 - val_accuracy: 0.9040\n",
      "Epoch 85/100\n",
      "79/82 [===========================>..] - ETA: 0s - loss: 0.3849 - accuracy: 0.8639\n",
      "Epoch 85: val_loss did not improve from 0.22319\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.3916 - accuracy: 0.8628 - val_loss: 0.2320 - val_accuracy: 0.9055\n",
      "Epoch 86/100\n",
      "58/82 [====================>.........] - ETA: 0s - loss: 0.3628 - accuracy: 0.8664\n",
      "Epoch 86: val_loss did not improve from 0.22319\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.3540 - accuracy: 0.8716 - val_loss: 0.2383 - val_accuracy: 0.9055\n",
      "Epoch 87/100\n",
      "57/82 [===================>..........] - ETA: 0s - loss: 0.3544 - accuracy: 0.8712\n",
      "Epoch 87: val_loss did not improve from 0.22319\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.3502 - accuracy: 0.8742 - val_loss: 0.2326 - val_accuracy: 0.9009\n",
      "Epoch 88/100\n",
      "56/82 [===================>..........] - ETA: 0s - loss: 0.3345 - accuracy: 0.8744\n",
      "Epoch 88: val_loss improved from 0.22319 to 0.21777, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.3398 - accuracy: 0.8735 - val_loss: 0.2178 - val_accuracy: 0.9101\n",
      "Epoch 89/100\n",
      "56/82 [===================>..........] - ETA: 0s - loss: 0.3298 - accuracy: 0.8778\n",
      "Epoch 89: val_loss did not improve from 0.21777\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.3631 - accuracy: 0.8670 - val_loss: 0.2475 - val_accuracy: 0.9040\n",
      "Epoch 90/100\n",
      "53/82 [==================>...........] - ETA: 0s - loss: 0.3528 - accuracy: 0.8638\n",
      "Epoch 90: val_loss did not improve from 0.21777\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.3431 - accuracy: 0.8704 - val_loss: 0.2485 - val_accuracy: 0.9040\n",
      "Epoch 91/100\n",
      "56/82 [===================>..........] - ETA: 0s - loss: 0.3262 - accuracy: 0.8806\n",
      "Epoch 91: val_loss did not improve from 0.21777\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.3272 - accuracy: 0.8784 - val_loss: 0.2214 - val_accuracy: 0.9070\n",
      "Epoch 92/100\n",
      "56/82 [===================>..........] - ETA: 0s - loss: 0.3429 - accuracy: 0.8700\n",
      "Epoch 92: val_loss did not improve from 0.21777\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.3361 - accuracy: 0.8723 - val_loss: 0.2405 - val_accuracy: 0.9055\n",
      "Epoch 93/100\n",
      "64/82 [======================>.......] - ETA: 0s - loss: 0.3374 - accuracy: 0.8760\n",
      "Epoch 93: val_loss improved from 0.21777 to 0.20415, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.3289 - accuracy: 0.8815 - val_loss: 0.2041 - val_accuracy: 0.9146\n",
      "Epoch 94/100\n",
      "61/82 [=====================>........] - ETA: 0s - loss: 0.3123 - accuracy: 0.8837\n",
      "Epoch 94: val_loss did not improve from 0.20415\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.3144 - accuracy: 0.8819 - val_loss: 0.2070 - val_accuracy: 0.9116\n",
      "Epoch 95/100\n",
      "54/82 [==================>...........] - ETA: 0s - loss: 0.3825 - accuracy: 0.8600\n",
      "Epoch 95: val_loss did not improve from 0.20415\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.3771 - accuracy: 0.8632 - val_loss: 0.2403 - val_accuracy: 0.8979\n",
      "Epoch 96/100\n",
      "58/82 [====================>.........] - ETA: 0s - loss: 0.3178 - accuracy: 0.8842\n",
      "Epoch 96: val_loss did not improve from 0.20415\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.3171 - accuracy: 0.8849 - val_loss: 0.2238 - val_accuracy: 0.9070\n",
      "Epoch 97/100\n",
      "52/82 [==================>...........] - ETA: 0s - loss: 0.3353 - accuracy: 0.8708\n",
      "Epoch 97: val_loss did not improve from 0.20415\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.3252 - accuracy: 0.8773 - val_loss: 0.2313 - val_accuracy: 0.9055\n",
      "Epoch 98/100\n",
      "55/82 [===================>..........] - ETA: 0s - loss: 0.3759 - accuracy: 0.8670\n",
      "Epoch 98: val_loss did not improve from 0.20415\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.3529 - accuracy: 0.8712 - val_loss: 0.2206 - val_accuracy: 0.9116\n",
      "Epoch 99/100\n",
      "55/82 [===================>..........] - ETA: 0s - loss: 0.3115 - accuracy: 0.8807\n",
      "Epoch 99: val_loss did not improve from 0.20415\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.3111 - accuracy: 0.8845 - val_loss: 0.2263 - val_accuracy: 0.9055\n",
      "Epoch 100/100\n",
      "56/82 [===================>..........] - ETA: 0s - loss: 0.3326 - accuracy: 0.8717\n",
      "Epoch 100: val_loss did not improve from 0.20415\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.3299 - accuracy: 0.8727 - val_loss: 0.2042 - val_accuracy: 0.9146\n",
      "0:00:23.403666\n"
     ]
    }
   ],
   "source": [
    "#Train the model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from datetime import datetime\n",
    "\n",
    "num_epochs=100\n",
    "num_batch_size=32\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='audio_classification.hdf5',verbose=1,save_best_only=True)\n",
    "start = datetime.now()\n",
    "\n",
    "model.fit(X_train,y_train,batch_size = num_batch_size,epochs=num_epochs,validation_data=(X_test,y_test),callbacks=[checkpointer])\n",
    "\n",
    "duration = datetime.now()-start\n",
    "print(duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f533918b-997e-447f-8214-324ddcbc297d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91 %\n"
     ]
    }
   ],
   "source": [
    "test_accuracy=model.evaluate(X_test,y_test,verbose=0)\n",
    "print(round(test_accuracy[1]*100),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c02c3d6e-d75c-4972-9b65-1a7e9b9793ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predictEmotion(filename):\n",
    "        audio, sample_rate = librosa.load(filename, res_type='kaiser_fast') \n",
    "        mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "        mfccs_scaled_features = np.mean(mfccs_features.T,axis=0)\n",
    "\n",
    "        #print(mfccs_scaled_features)\n",
    "        mfccs_scaled_features=mfccs_scaled_features.reshape(1,-1)\n",
    "        #print(mfccs_scaled_features)\n",
    "        #print(mfccs_scaled_features.shape)\n",
    "        predicted_label=model.predict(mfccs_scaled_features)\n",
    "        predicted_label=np.argmax(predicted_label,axis=-1)\n",
    "        print(predicted_label)\n",
    "        prediction_class = labelencoder.inverse_transform(predicted_label) \n",
    "        print(prediction_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4a53ae",
   "metadata": {},
   "source": [
    "# Prediction for samples in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "605df6fb-5699-48f7-b61a-579d1c5fdcff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 96ms/step\n",
      "[6]\n",
      "['surprise']\n"
     ]
    }
   ],
   "source": [
    "predictEmotion('YAF_bite_ps.wav')# surpise file (female)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ef95fc32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 43ms/step\n",
      "[3]\n",
      "['happy']\n"
     ]
    }
   ],
   "source": [
    "predictEmotion('DC_a01.wav')#angry file (Male)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e8db5a27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 49ms/step\n",
      "[1]\n",
      "['disgust']\n"
     ]
    }
   ],
   "source": [
    "predictEmotion('OAF_cab_sad.wav')#sad file (female)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "38d188f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 35ms/step\n",
      "[3]\n",
      "['happy']\n"
     ]
    }
   ],
   "source": [
    "predictEmotion('YAF_rose_happy.wav')#happy file (female)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5b75ee2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 30ms/step\n",
      "[4]\n",
      "['neutral']\n"
     ]
    }
   ],
   "source": [
    "predictEmotion('DC_n24.wav')#neutral (Male)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4a421c",
   "metadata": {},
   "source": [
    "# Prediction for samples not in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7e4ee4ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 34ms/step\n",
      "[3]\n",
      "['happy']\n"
     ]
    }
   ],
   "source": [
    "predictEmotion('03-01-08-01-01-01-24.wav')# surprised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f5285112",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 48ms/step\n",
      "[3]\n",
      "['happy']\n"
     ]
    }
   ],
   "source": [
    "predictEmotion('03-01-08-01-01-01-24.wav')# fearful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1182a46c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 40ms/step\n",
      "[1]\n",
      "['disgust']\n"
     ]
    }
   ],
   "source": [
    "predictEmotion('03-01-01-01-01-01-10.wav')#neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "60063ad8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 45ms/step\n",
      "[3]\n",
      "['happy']\n"
     ]
    }
   ],
   "source": [
    "predictEmotion('03-01-05-01-01-01-05.wav')#angry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "28afae1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 46ms/step\n",
      "[1]\n",
      "['disgust']\n"
     ]
    }
   ],
   "source": [
    "predictEmotion('03-01-03-01-01-01-17.wav')#happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38586754",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
