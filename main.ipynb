{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "681db7de-56df-408d-8dd9-cba3cc2ded66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2044661c-e7ea-4db6-ab45-8afc383f1330",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9ebbcfb-9d28-4019-b725-28fe7875e782",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SAVEE = './dataset/ALL/'\n",
    "dir_list = os.listdir(SAVEE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e809d054-2ac3-4243-99de-88f09da2342c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "emotion=[]\n",
    "path = []\n",
    "for i in dir_list:\n",
    "    if i[-8:-6]=='_a':\n",
    "        emotion.append('angry')\n",
    "    elif i[-8:-6]=='_d':\n",
    "        emotion.append('disgust')\n",
    "    elif i[-8:-6]=='_f':\n",
    "        emotion.append('fear')\n",
    "    elif i[-8:-6]=='_h':\n",
    "        emotion.append('happy')\n",
    "    elif i[-8:-6]=='_n':\n",
    "        emotion.append('neutral')\n",
    "    elif i[-8:-6]=='sa':\n",
    "        emotion.append('sad')\n",
    "    elif i[-8:-6]=='su':\n",
    "        emotion.append('surprise')\n",
    "    else:\n",
    "        emotion.append('unknown') \n",
    "    path.append(SAVEE + i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "980cde5f-fc9c-489e-9293-fe20d0ab1a19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVEE dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>angry</td>\n",
       "      <td>./dataset/ALL/DC_a01.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>angry</td>\n",
       "      <td>./dataset/ALL/DC_a02.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>angry</td>\n",
       "      <td>./dataset/ALL/DC_a03.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>angry</td>\n",
       "      <td>./dataset/ALL/DC_a04.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>angry</td>\n",
       "      <td>./dataset/ALL/DC_a05.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  labels                      path\n",
       "0  angry  ./dataset/ALL/DC_a01.wav\n",
       "1  angry  ./dataset/ALL/DC_a02.wav\n",
       "2  angry  ./dataset/ALL/DC_a03.wav\n",
       "3  angry  ./dataset/ALL/DC_a04.wav\n",
       "4  angry  ./dataset/ALL/DC_a05.wav"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAVEE_df = pd.DataFrame(emotion, columns = ['labels'])\n",
    "SAVEE_df = pd.concat([SAVEE_df, pd.DataFrame(path, columns = ['path'])], axis = 1)\n",
    "print('SAVEE dataset')\n",
    "SAVEE_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1de31143-1520-402c-9db3-8c15534ef058",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TESS = './dataset/TESS Toronto emotional speech set data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bee0fbfd-2302-4177-98ca-751f7d371fcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = []\n",
    "emotion = []\n",
    "dir_list = os.listdir(TESS)\n",
    "\n",
    "for i in dir_list:\n",
    "    fname = os.listdir(TESS + i)   \n",
    "    for f in fname:\n",
    "        if i == 'OAF_angry' or i == 'YAF_angry':\n",
    "            emotion.append('angry')\n",
    "        elif i == 'OAF_disgust' or i == 'YAF_disgust':\n",
    "            emotion.append('disgust')\n",
    "        elif i == 'OAF_Fear' or i == 'YAF_fear':\n",
    "            emotion.append('fear')\n",
    "        elif i == 'OAF_happy' or i == 'YAF_happy':\n",
    "            emotion.append('happy')\n",
    "        elif i == 'OAF_neutral' or i == 'YAF_neutral':\n",
    "            emotion.append('neutral')                                \n",
    "        elif i == 'OAF_Pleasant_surprise' or i == 'YAF_pleasant_surprised':\n",
    "            emotion.append('surprise')               \n",
    "        elif i == 'OAF_Sad' or i == 'YAF_sad':\n",
    "            emotion.append('sad')\n",
    "        else:\n",
    "            emotion.append('Unknown')\n",
    "        path.append(TESS + i + \"/\" + f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fce0112-5797-4bb1-90be-65f2ca7e8098",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESS dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>angry</td>\n",
       "      <td>./dataset/TESS Toronto emotional speech set da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>angry</td>\n",
       "      <td>./dataset/TESS Toronto emotional speech set da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>angry</td>\n",
       "      <td>./dataset/TESS Toronto emotional speech set da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>angry</td>\n",
       "      <td>./dataset/TESS Toronto emotional speech set da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>angry</td>\n",
       "      <td>./dataset/TESS Toronto emotional speech set da...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  labels                                               path\n",
       "0  angry  ./dataset/TESS Toronto emotional speech set da...\n",
       "1  angry  ./dataset/TESS Toronto emotional speech set da...\n",
       "2  angry  ./dataset/TESS Toronto emotional speech set da...\n",
       "3  angry  ./dataset/TESS Toronto emotional speech set da...\n",
       "4  angry  ./dataset/TESS Toronto emotional speech set da..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TESS_df = pd.DataFrame(emotion, columns = ['labels'])\n",
    "#TESS_df['source'] = 'TESS'\n",
    "TESS_df = pd.concat([TESS_df,pd.DataFrame(path, columns = ['path'])],axis=1)\n",
    "print('TESS dataset')\n",
    "TESS_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "314f9cd3-557c-4c97-8942-cbbc7f685c0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now lets merge all the dataframe\n",
    "Males = pd.concat([SAVEE_df], axis = 0)\n",
    "Males.to_csv(\"males_emotions_df.csv\", index = False)\n",
    "\n",
    "Females = pd.concat([TESS_df], axis = 0)\n",
    "Females.to_csv(\"females_emotions_df.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "233a50ec-acbd-4b99-881e-a7eaf40f8119",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "comb = [Males,Females]\n",
    "Dataset=pd.concat(comb)\n",
    "Dataset=Dataset.sort_values(by=['labels'])\n",
    "Dataset.to_csv(\"Dataset_df.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe24779-08f0-49fa-8f40-79f76647b68a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "224c4b78-eea9-40af-a1e2-d7951c40fdcf",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6f0ffe7-6225-458b-b0c4-1b1e9b0e3921",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.utils import to_categorical \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder=LabelEncoder()\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49186158-f690-4a3a-af92-0ac91b95c2cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_extractor(file):\n",
    "    data,sample_rate=librosa.load(file,res_type=\"kaiser_fast\")\n",
    "    \n",
    "    #MFCC \n",
    "    mfccs_features = librosa.feature.mfcc(y=data,sr=sample_rate,n_mfcc=40)\n",
    "    mfccs_scaled_features = np.mean(mfccs_features.T,axis=0)\n",
    "    \n",
    "    # Chroma_stft\n",
    "    stft = np.abs(librosa.stft(data))\n",
    "    chroma_stft = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n",
    "    mfccs_scaled_features = np.hstack((mfccs_scaled_features, chroma_stft)) # stacking horizontally\n",
    "    \n",
    "    #zcr\n",
    "    zcr = np.mean(librosa.feature.zero_crossing_rate(y=data).T, axis=0)\n",
    "    mfccs_scaled_features=np.hstack((mfccs_scaled_features, zcr))\n",
    "    \n",
    "    #rms\n",
    "    rms = np.mean(librosa.feature.rms(y=data).T, axis=0)\n",
    "    mfccs_scaled_features = np.hstack((mfccs_scaled_features, rms)) # stacking horizontally\n",
    "\n",
    "    # MelSpectogram\n",
    "    mel = np.mean(librosa.feature.melspectrogram(y=data, sr=sample_rate).T, axis=0)\n",
    "    mfccs_scaled_features = np.hstack((mfccs_scaled_features, mel)) # stacking horizontally\n",
    "\n",
    "    return mfccs_scaled_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90a9997a-1999-498b-8512-4b91692ad926",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3280it [04:54, 11.13it/s]\n"
     ]
    }
   ],
   "source": [
    "extracted_features=[]\n",
    "for index_num,row in tqdm(Dataset.iterrows()):\n",
    "    file_name = os.path.join(str(row[\"path\"]))\n",
    "    final_class_labels=row['labels']\n",
    "    data=feature_extractor(file_name)\n",
    "    extracted_features.append([data,final_class_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35c521d4-a230-43c9-a8e2-19e81be11d6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.18135529e+02  1.12588829e+02  1.23745937e+01  3.45954971e+01\n",
      "  1.16200695e+01 -2.91651773e+00 -2.48573990e+01 -2.59529781e+00\n",
      " -1.00110602e+00 -1.15826321e+01 -2.48748541e+00 -9.69559789e-01\n",
      " -9.28020418e-01 -1.63937509e+00  5.17986345e+00  6.59545600e-01\n",
      " -3.40372515e+00  5.59151697e+00 -9.27957833e-01 -5.73660564e+00\n",
      "  5.74505851e-02  5.40333331e-01  1.78466511e+00 -2.08028388e+00\n",
      " -1.25319338e+00 -3.53276229e+00 -4.19973946e+00 -1.58740610e-01\n",
      " -2.00895476e+00  5.86389303e-01  1.40789413e+00  1.71946561e+00\n",
      "  3.23082638e+00  2.73758245e+00  3.46699786e+00  5.05381870e+00\n",
      "  4.83345985e+00  5.60282660e+00  4.30647087e+00  3.39099669e+00\n",
      "  5.69994271e-01  5.91923118e-01  5.67894936e-01  5.37071347e-01\n",
      "  5.34729660e-01  5.55286288e-01  5.55979609e-01  5.29761136e-01\n",
      "  5.86599410e-01  6.23893976e-01  5.56559861e-01  5.35161257e-01\n",
      "  2.77331388e-02  1.33699149e-01  7.72051907e+00  2.03712527e-02\n",
      "  2.36469712e-02  8.19036961e-01  1.75192738e+01  2.15448666e+01\n",
      "  1.70417671e+01  1.39207258e+01  4.24016809e+00  1.98538227e+01\n",
      "  3.69275475e+01  2.07941532e+01  2.73107452e+01  5.23219566e+01\n",
      "  5.50042267e+01  1.09524155e+02  1.20045464e+02  4.42795143e+01\n",
      "  2.14670696e+01  2.34186401e+01  4.28324623e+01  3.24327240e+01\n",
      "  1.63621101e+01  5.44345379e+00  8.15269756e+00  1.25052814e+01\n",
      "  6.34266043e+00  3.72413278e+00  2.59380555e+00  1.98195422e+00\n",
      "  1.11523461e+00  7.47613251e-01  1.14031959e+00  9.95153904e-01\n",
      "  1.33376861e+00  6.66728318e-01  2.59138077e-01  3.17626953e-01\n",
      "  5.38871706e-01  4.84656364e-01  2.03734770e-01  2.50190288e-01\n",
      "  2.98724413e-01  2.98103422e-01  2.90976912e-01  9.67743695e-02\n",
      "  1.09226622e-01  1.87975824e-01  1.82121247e-01  8.03377181e-02\n",
      "  4.65054810e-02  1.58078611e-01  1.56605691e-01  1.34628832e-01\n",
      "  1.42365411e-01  1.57854304e-01  2.43386716e-01  2.35424593e-01\n",
      "  2.09963381e-01  1.45023093e-01  1.21320963e-01  9.67353508e-02\n",
      "  1.23003043e-01  1.21778756e-01  9.33960378e-02  8.29376355e-02\n",
      "  9.94628817e-02  8.34799856e-02  9.34564322e-02  1.57322049e-01\n",
      "  2.36535326e-01  2.79894382e-01  2.56242782e-01  1.50927961e-01\n",
      "  1.61290318e-01  6.63402155e-02  3.60290743e-02  1.33868139e-02\n",
      "  7.14542903e-03  8.45604483e-03  6.76299818e-03  1.91014390e-02\n",
      "  2.79310830e-02  1.55688990e-02  2.35105064e-02  1.77454725e-02\n",
      "  9.86930262e-03  1.46401664e-02  2.32467540e-02  2.11503282e-02\n",
      "  2.81682834e-02  4.29636501e-02  3.86513025e-02  2.56419964e-02\n",
      "  3.20835896e-02  4.44187075e-02  2.50618253e-02  1.68933515e-02\n",
      "  1.06134703e-02  6.33903686e-03  6.69169333e-03  1.01197204e-02\n",
      "  1.48856426e-02  1.64814759e-02  1.58254188e-02  1.36664091e-02\n",
      "  1.06785987e-02  7.06947921e-03  2.66686012e-03  1.55878684e-03\n",
      "  8.01483460e-04  3.91695619e-04  4.19313496e-04  4.17181669e-04\n",
      "  3.42274609e-04  2.37085042e-04  1.13650734e-04  5.93935401e-05\n",
      "  2.02277988e-05  5.11281996e-06  1.17555749e-06  6.33837033e-07\n",
      "  5.14800320e-07  4.19386936e-07  3.33630652e-07  2.68055288e-07\n",
      "  2.30305375e-07  2.13139074e-07]\n"
     ]
    }
   ],
   "source": [
    "extracted_features_df = pd.DataFrame(extracted_features,columns=['feature','class'])\n",
    "extracted_features_df.head()\n",
    "print(extracted_features_df.iloc[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "47d7567d-2f02-47d6-865a-54c140c6c381",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#split in indep and dep dataset\n",
    "X = np.array(extracted_features_df['feature'].tolist())\n",
    "y = np.array(extracted_features_df['class'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a012ffac-b3bf-4d78-845b-d72c45dabe68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y=to_categorical(labelencoder.fit_transform(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2b2db96e-d88f-46a3-a884-e4d6ff7ab6eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dc8eee-1846-4324-aa4e-147a18a65178",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a203bc12-4341-43f6-a524-4cf2164233d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,Activation,Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4eba89cf-2f7a-41f0-b8f7-b8a7f5a05c8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_labels = y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "372e780f-7a0e-414f-9e8f-a31583e029fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "#first layer\n",
    "model.add(Dense(100,input_shape=(182,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "#second layer\n",
    "model.add(Dense(200))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "#third layer\n",
    "model.add(Dense(100))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#final layer\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "39de56ef-d1a0-4ebd-ad41-bf8ae7bbcb25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_16 (Dense)            (None, 100)               18300     \n",
      "                                                                 \n",
      " activation_16 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 100)               0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 200)               20200     \n",
      "                                                                 \n",
      " activation_17 (Activation)  (None, 200)               0         \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 200)               0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 100)               20100     \n",
      "                                                                 \n",
      " activation_18 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 100)               0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 7)                 707       \n",
      "                                                                 \n",
      " activation_19 (Activation)  (None, 7)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 59,307\n",
      "Trainable params: 59,307\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9e4de4c3-af87-44fb-8c5d-e891079333b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dba8632-4167-4a39-ad6c-ad14c4d01e78",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "237775e8-52ac-45ec-89aa-7a89b5e1a531",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "72/82 [=========================>....] - ETA: 0s - loss: 21.8192 - accuracy: 0.1706\n",
      "Epoch 1: val_loss improved from inf to 1.95599, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 2s 11ms/step - loss: 20.0720 - accuracy: 0.1704 - val_loss: 1.9560 - val_accuracy: 0.1799\n",
      "Epoch 2/100\n",
      "76/82 [==========================>...] - ETA: 0s - loss: 4.6495 - accuracy: 0.1706\n",
      "Epoch 2: val_loss improved from 1.95599 to 1.94561, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 1s 6ms/step - loss: 4.5222 - accuracy: 0.1711 - val_loss: 1.9456 - val_accuracy: 0.1326\n",
      "Epoch 3/100\n",
      "76/82 [==========================>...] - ETA: 0s - loss: 2.6651 - accuracy: 0.1920\n",
      "Epoch 3: val_loss improved from 1.94561 to 1.93705, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 2.6208 - accuracy: 0.1951 - val_loss: 1.9370 - val_accuracy: 0.2119\n",
      "Epoch 4/100\n",
      "71/82 [========================>.....] - ETA: 0s - loss: 2.2189 - accuracy: 0.2069\n",
      "Epoch 4: val_loss improved from 1.93705 to 1.88204, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 2.2042 - accuracy: 0.2062 - val_loss: 1.8820 - val_accuracy: 0.3049\n",
      "Epoch 5/100\n",
      "78/82 [===========================>..] - ETA: 0s - loss: 1.9234 - accuracy: 0.2492\n",
      "Epoch 5: val_loss improved from 1.88204 to 1.73907, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 1.9201 - accuracy: 0.2496 - val_loss: 1.7391 - val_accuracy: 0.3796\n",
      "Epoch 6/100\n",
      "77/82 [===========================>..] - ETA: 0s - loss: 1.8238 - accuracy: 0.2886\n",
      "Epoch 6: val_loss improved from 1.73907 to 1.63680, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 1.8243 - accuracy: 0.2881 - val_loss: 1.6368 - val_accuracy: 0.4268\n",
      "Epoch 7/100\n",
      "74/82 [==========================>...] - ETA: 0s - loss: 1.7319 - accuracy: 0.3311\n",
      "Epoch 7: val_loss improved from 1.63680 to 1.54148, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 1.7235 - accuracy: 0.3323 - val_loss: 1.5415 - val_accuracy: 0.4604\n",
      "Epoch 8/100\n",
      "78/82 [===========================>..] - ETA: 0s - loss: 1.6146 - accuracy: 0.3910\n",
      "Epoch 8: val_loss improved from 1.54148 to 1.40690, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 1.6125 - accuracy: 0.3891 - val_loss: 1.4069 - val_accuracy: 0.5884\n",
      "Epoch 9/100\n",
      "75/82 [==========================>...] - ETA: 0s - loss: 1.5047 - accuracy: 0.4025\n",
      "Epoch 9: val_loss improved from 1.40690 to 1.31344, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 1.5057 - accuracy: 0.4066 - val_loss: 1.3134 - val_accuracy: 0.6357\n",
      "Epoch 10/100\n",
      "76/82 [==========================>...] - ETA: 0s - loss: 1.4142 - accuracy: 0.4696\n",
      "Epoch 10: val_loss improved from 1.31344 to 1.16048, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 1.4131 - accuracy: 0.4707 - val_loss: 1.1605 - val_accuracy: 0.6905\n",
      "Epoch 11/100\n",
      "75/82 [==========================>...] - ETA: 0s - loss: 1.3301 - accuracy: 0.5108\n",
      "Epoch 11: val_loss improved from 1.16048 to 1.06506, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 1.3303 - accuracy: 0.5091 - val_loss: 1.0651 - val_accuracy: 0.7210\n",
      "Epoch 12/100\n",
      "74/82 [==========================>...] - ETA: 0s - loss: 1.2121 - accuracy: 0.5608\n",
      "Epoch 12: val_loss improved from 1.06506 to 0.98085, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 1.2081 - accuracy: 0.5591 - val_loss: 0.9809 - val_accuracy: 0.7591\n",
      "Epoch 13/100\n",
      "73/82 [=========================>....] - ETA: 0s - loss: 1.1412 - accuracy: 0.5719\n",
      "Epoch 13: val_loss improved from 0.98085 to 0.87123, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 1.1394 - accuracy: 0.5713 - val_loss: 0.8712 - val_accuracy: 0.7683\n",
      "Epoch 14/100\n",
      "79/82 [===========================>..] - ETA: 0s - loss: 1.0844 - accuracy: 0.6036\n",
      "Epoch 14: val_loss improved from 0.87123 to 0.76702, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 1.0823 - accuracy: 0.6044 - val_loss: 0.7670 - val_accuracy: 0.7790\n",
      "Epoch 15/100\n",
      "80/82 [============================>.] - ETA: 0s - loss: 0.9875 - accuracy: 0.6438\n",
      "Epoch 15: val_loss improved from 0.76702 to 0.70116, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.9877 - accuracy: 0.6437 - val_loss: 0.7012 - val_accuracy: 0.7896\n",
      "Epoch 16/100\n",
      "74/82 [==========================>...] - ETA: 0s - loss: 0.9164 - accuracy: 0.6689\n",
      "Epoch 16: val_loss improved from 0.70116 to 0.64421, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.9260 - accuracy: 0.6654 - val_loss: 0.6442 - val_accuracy: 0.8095\n",
      "Epoch 17/100\n",
      "74/82 [==========================>...] - ETA: 0s - loss: 0.8882 - accuracy: 0.7006\n",
      "Epoch 17: val_loss improved from 0.64421 to 0.59540, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.8816 - accuracy: 0.6997 - val_loss: 0.5954 - val_accuracy: 0.8308\n",
      "Epoch 18/100\n",
      "75/82 [==========================>...] - ETA: 0s - loss: 0.8250 - accuracy: 0.6971\n",
      "Epoch 18: val_loss improved from 0.59540 to 0.56682, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.8190 - accuracy: 0.6982 - val_loss: 0.5668 - val_accuracy: 0.8415\n",
      "Epoch 19/100\n",
      "76/82 [==========================>...] - ETA: 0s - loss: 0.7762 - accuracy: 0.7204\n",
      "Epoch 19: val_loss improved from 0.56682 to 0.54198, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.7816 - accuracy: 0.7195 - val_loss: 0.5420 - val_accuracy: 0.8369\n",
      "Epoch 20/100\n",
      "75/82 [==========================>...] - ETA: 0s - loss: 0.7806 - accuracy: 0.7275\n",
      "Epoch 20: val_loss improved from 0.54198 to 0.49166, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.7800 - accuracy: 0.7306 - val_loss: 0.4917 - val_accuracy: 0.8308\n",
      "Epoch 21/100\n",
      "80/82 [============================>.] - ETA: 0s - loss: 0.7383 - accuracy: 0.7402\n",
      "Epoch 21: val_loss improved from 0.49166 to 0.44898, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.7384 - accuracy: 0.7389 - val_loss: 0.4490 - val_accuracy: 0.8415\n",
      "Epoch 22/100\n",
      "73/82 [=========================>....] - ETA: 0s - loss: 0.7002 - accuracy: 0.7547\n",
      "Epoch 22: val_loss did not improve from 0.44898\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.6977 - accuracy: 0.7538 - val_loss: 0.4491 - val_accuracy: 0.8491\n",
      "Epoch 23/100\n",
      "72/82 [=========================>....] - ETA: 0s - loss: 0.6610 - accuracy: 0.7700\n",
      "Epoch 23: val_loss improved from 0.44898 to 0.43815, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.6539 - accuracy: 0.7710 - val_loss: 0.4381 - val_accuracy: 0.8720\n",
      "Epoch 24/100\n",
      "74/82 [==========================>...] - ETA: 0s - loss: 0.6637 - accuracy: 0.7572\n",
      "Epoch 24: val_loss improved from 0.43815 to 0.42705, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.6588 - accuracy: 0.7611 - val_loss: 0.4271 - val_accuracy: 0.8933\n",
      "Epoch 25/100\n",
      "78/82 [===========================>..] - ETA: 0s - loss: 0.5846 - accuracy: 0.7845\n",
      "Epoch 25: val_loss improved from 0.42705 to 0.36278, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.5842 - accuracy: 0.7851 - val_loss: 0.3628 - val_accuracy: 0.8902\n",
      "Epoch 26/100\n",
      "73/82 [=========================>....] - ETA: 0s - loss: 0.5854 - accuracy: 0.8014\n",
      "Epoch 26: val_loss improved from 0.36278 to 0.33781, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.5802 - accuracy: 0.8022 - val_loss: 0.3378 - val_accuracy: 0.8918\n",
      "Epoch 27/100\n",
      "75/82 [==========================>...] - ETA: 0s - loss: 0.5568 - accuracy: 0.8000\n",
      "Epoch 27: val_loss did not improve from 0.33781\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.5592 - accuracy: 0.8003 - val_loss: 0.3524 - val_accuracy: 0.8826\n",
      "Epoch 28/100\n",
      "72/82 [=========================>....] - ETA: 0s - loss: 0.5802 - accuracy: 0.8021\n",
      "Epoch 28: val_loss improved from 0.33781 to 0.32898, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.5688 - accuracy: 0.8045 - val_loss: 0.3290 - val_accuracy: 0.8918\n",
      "Epoch 29/100\n",
      "81/82 [============================>.] - ETA: 0s - loss: 0.5367 - accuracy: 0.8171\n",
      "Epoch 29: val_loss improved from 0.32898 to 0.29780, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.5395 - accuracy: 0.8152 - val_loss: 0.2978 - val_accuracy: 0.9055\n",
      "Epoch 30/100\n",
      "73/82 [=========================>....] - ETA: 0s - loss: 0.5371 - accuracy: 0.8185\n",
      "Epoch 30: val_loss did not improve from 0.29780\n",
      "82/82 [==============================] - 0s 4ms/step - loss: 0.5268 - accuracy: 0.8201 - val_loss: 0.3221 - val_accuracy: 0.8963\n",
      "Epoch 31/100\n",
      "67/82 [=======================>......] - ETA: 0s - loss: 0.4959 - accuracy: 0.8279\n",
      "Epoch 31: val_loss improved from 0.29780 to 0.28293, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 4ms/step - loss: 0.4857 - accuracy: 0.8312 - val_loss: 0.2829 - val_accuracy: 0.8979\n",
      "Epoch 32/100\n",
      "74/82 [==========================>...] - ETA: 0s - loss: 0.4739 - accuracy: 0.8319\n",
      "Epoch 32: val_loss did not improve from 0.28293\n",
      "82/82 [==============================] - 0s 4ms/step - loss: 0.4750 - accuracy: 0.8335 - val_loss: 0.3249 - val_accuracy: 0.8979\n",
      "Epoch 33/100\n",
      "68/82 [=======================>......] - ETA: 0s - loss: 0.4692 - accuracy: 0.8281\n",
      "Epoch 33: val_loss did not improve from 0.28293\n",
      "82/82 [==============================] - 0s 4ms/step - loss: 0.4744 - accuracy: 0.8293 - val_loss: 0.2949 - val_accuracy: 0.8979\n",
      "Epoch 34/100\n",
      "69/82 [========================>.....] - ETA: 0s - loss: 0.4610 - accuracy: 0.8474\n",
      "Epoch 34: val_loss did not improve from 0.28293\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.4583 - accuracy: 0.8479 - val_loss: 0.2881 - val_accuracy: 0.8902\n",
      "Epoch 35/100\n",
      "67/82 [=======================>......] - ETA: 0s - loss: 0.4779 - accuracy: 0.8391\n",
      "Epoch 35: val_loss improved from 0.28293 to 0.26962, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 4ms/step - loss: 0.4741 - accuracy: 0.8373 - val_loss: 0.2696 - val_accuracy: 0.9070\n",
      "Epoch 36/100\n",
      "71/82 [========================>.....] - ETA: 0s - loss: 0.4481 - accuracy: 0.8486\n",
      "Epoch 36: val_loss improved from 0.26962 to 0.26133, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.4491 - accuracy: 0.8460 - val_loss: 0.2613 - val_accuracy: 0.9024\n",
      "Epoch 37/100\n",
      "74/82 [==========================>...] - ETA: 0s - loss: 0.4662 - accuracy: 0.8399\n",
      "Epoch 37: val_loss did not improve from 0.26133\n",
      "82/82 [==============================] - 0s 4ms/step - loss: 0.4644 - accuracy: 0.8388 - val_loss: 0.2638 - val_accuracy: 0.9024\n",
      "Epoch 38/100\n",
      "66/82 [=======================>......] - ETA: 0s - loss: 0.4464 - accuracy: 0.8494\n",
      "Epoch 38: val_loss improved from 0.26133 to 0.25690, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.4357 - accuracy: 0.8518 - val_loss: 0.2569 - val_accuracy: 0.9085\n",
      "Epoch 39/100\n",
      "75/82 [==========================>...] - ETA: 0s - loss: 0.4148 - accuracy: 0.8596\n",
      "Epoch 39: val_loss did not improve from 0.25690\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.4177 - accuracy: 0.8567 - val_loss: 0.2587 - val_accuracy: 0.9009\n",
      "Epoch 40/100\n",
      "78/82 [===========================>..] - ETA: 0s - loss: 0.3892 - accuracy: 0.8662\n",
      "Epoch 40: val_loss improved from 0.25690 to 0.23783, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.3919 - accuracy: 0.8655 - val_loss: 0.2378 - val_accuracy: 0.9085\n",
      "Epoch 41/100\n",
      "74/82 [==========================>...] - ETA: 0s - loss: 0.3907 - accuracy: 0.8699\n",
      "Epoch 41: val_loss improved from 0.23783 to 0.23675, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.3855 - accuracy: 0.8704 - val_loss: 0.2367 - val_accuracy: 0.9070\n",
      "Epoch 42/100\n",
      "72/82 [=========================>....] - ETA: 0s - loss: 0.4459 - accuracy: 0.8533\n",
      "Epoch 42: val_loss improved from 0.23675 to 0.23373, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.4320 - accuracy: 0.8594 - val_loss: 0.2337 - val_accuracy: 0.9116\n",
      "Epoch 43/100\n",
      "78/82 [===========================>..] - ETA: 0s - loss: 0.3990 - accuracy: 0.8690\n",
      "Epoch 43: val_loss did not improve from 0.23373\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.3988 - accuracy: 0.8678 - val_loss: 0.2517 - val_accuracy: 0.8994\n",
      "Epoch 44/100\n",
      "76/82 [==========================>...] - ETA: 0s - loss: 0.4037 - accuracy: 0.8573\n",
      "Epoch 44: val_loss did not improve from 0.23373\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.4010 - accuracy: 0.8579 - val_loss: 0.2425 - val_accuracy: 0.9055\n",
      "Epoch 45/100\n",
      "75/82 [==========================>...] - ETA: 0s - loss: 0.3833 - accuracy: 0.8683\n",
      "Epoch 45: val_loss improved from 0.23373 to 0.23327, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.3819 - accuracy: 0.8697 - val_loss: 0.2333 - val_accuracy: 0.9101\n",
      "Epoch 46/100\n",
      "75/82 [==========================>...] - ETA: 0s - loss: 0.3822 - accuracy: 0.8775\n",
      "Epoch 46: val_loss did not improve from 0.23327\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.3806 - accuracy: 0.8765 - val_loss: 0.2340 - val_accuracy: 0.9085\n",
      "Epoch 47/100\n",
      "76/82 [==========================>...] - ETA: 0s - loss: 0.4031 - accuracy: 0.8627\n",
      "Epoch 47: val_loss did not improve from 0.23327\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.3995 - accuracy: 0.8628 - val_loss: 0.2586 - val_accuracy: 0.8963\n",
      "Epoch 48/100\n",
      "79/82 [===========================>..] - ETA: 0s - loss: 0.3747 - accuracy: 0.8738\n",
      "Epoch 48: val_loss did not improve from 0.23327\n",
      "82/82 [==============================] - 0s 4ms/step - loss: 0.3813 - accuracy: 0.8704 - val_loss: 0.2653 - val_accuracy: 0.9024\n",
      "Epoch 49/100\n",
      "77/82 [===========================>..] - ETA: 0s - loss: 0.3721 - accuracy: 0.8742\n",
      "Epoch 49: val_loss did not improve from 0.23327\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.3785 - accuracy: 0.8735 - val_loss: 0.2561 - val_accuracy: 0.8948\n",
      "Epoch 50/100\n",
      "76/82 [==========================>...] - ETA: 0s - loss: 0.3726 - accuracy: 0.8705\n",
      "Epoch 50: val_loss did not improve from 0.23327\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.3806 - accuracy: 0.8700 - val_loss: 0.2370 - val_accuracy: 0.9085\n",
      "Epoch 51/100\n",
      "77/82 [===========================>..] - ETA: 0s - loss: 0.3647 - accuracy: 0.8750\n",
      "Epoch 51: val_loss improved from 0.23327 to 0.22310, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.3655 - accuracy: 0.8731 - val_loss: 0.2231 - val_accuracy: 0.9162\n",
      "Epoch 52/100\n",
      "77/82 [===========================>..] - ETA: 0s - loss: 0.3682 - accuracy: 0.8738\n",
      "Epoch 52: val_loss did not improve from 0.22310\n",
      "82/82 [==============================] - 0s 4ms/step - loss: 0.3617 - accuracy: 0.8750 - val_loss: 0.2567 - val_accuracy: 0.9116\n",
      "Epoch 53/100\n",
      "75/82 [==========================>...] - ETA: 0s - loss: 0.3733 - accuracy: 0.8742\n",
      "Epoch 53: val_loss did not improve from 0.22310\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.3718 - accuracy: 0.8739 - val_loss: 0.2388 - val_accuracy: 0.9116\n",
      "Epoch 54/100\n",
      "75/82 [==========================>...] - ETA: 0s - loss: 0.3100 - accuracy: 0.8875\n",
      "Epoch 54: val_loss did not improve from 0.22310\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.3247 - accuracy: 0.8841 - val_loss: 0.2399 - val_accuracy: 0.8963\n",
      "Epoch 55/100\n",
      "77/82 [===========================>..] - ETA: 0s - loss: 0.3656 - accuracy: 0.8839\n",
      "Epoch 55: val_loss did not improve from 0.22310\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.3748 - accuracy: 0.8788 - val_loss: 0.2306 - val_accuracy: 0.9101\n",
      "Epoch 56/100\n",
      "77/82 [===========================>..] - ETA: 0s - loss: 0.3458 - accuracy: 0.8762\n",
      "Epoch 56: val_loss did not improve from 0.22310\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.3469 - accuracy: 0.8750 - val_loss: 0.2403 - val_accuracy: 0.9116\n",
      "Epoch 57/100\n",
      "75/82 [==========================>...] - ETA: 0s - loss: 0.3387 - accuracy: 0.8838\n",
      "Epoch 57: val_loss did not improve from 0.22310\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.3323 - accuracy: 0.8849 - val_loss: 0.2369 - val_accuracy: 0.9177\n",
      "Epoch 58/100\n",
      "77/82 [===========================>..] - ETA: 0s - loss: 0.3137 - accuracy: 0.8908\n",
      "Epoch 58: val_loss improved from 0.22310 to 0.20945, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.3223 - accuracy: 0.8883 - val_loss: 0.2094 - val_accuracy: 0.9162\n",
      "Epoch 59/100\n",
      "76/82 [==========================>...] - ETA: 0s - loss: 0.3028 - accuracy: 0.8935\n",
      "Epoch 59: val_loss did not improve from 0.20945\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.3013 - accuracy: 0.8941 - val_loss: 0.2332 - val_accuracy: 0.9146\n",
      "Epoch 60/100\n",
      "77/82 [===========================>..] - ETA: 0s - loss: 0.3392 - accuracy: 0.8835\n",
      "Epoch 60: val_loss did not improve from 0.20945\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.3359 - accuracy: 0.8853 - val_loss: 0.2244 - val_accuracy: 0.9101\n",
      "Epoch 61/100\n",
      "76/82 [==========================>...] - ETA: 0s - loss: 0.3280 - accuracy: 0.8873\n",
      "Epoch 61: val_loss did not improve from 0.20945\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.3262 - accuracy: 0.8876 - val_loss: 0.2348 - val_accuracy: 0.9116\n",
      "Epoch 62/100\n",
      "76/82 [==========================>...] - ETA: 0s - loss: 0.3573 - accuracy: 0.8820\n",
      "Epoch 62: val_loss did not improve from 0.20945\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.3598 - accuracy: 0.8796 - val_loss: 0.2331 - val_accuracy: 0.9040\n",
      "Epoch 63/100\n",
      "76/82 [==========================>...] - ETA: 0s - loss: 0.3436 - accuracy: 0.8840\n",
      "Epoch 63: val_loss did not improve from 0.20945\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.3426 - accuracy: 0.8838 - val_loss: 0.2378 - val_accuracy: 0.9116\n",
      "Epoch 64/100\n",
      "76/82 [==========================>...] - ETA: 0s - loss: 0.3049 - accuracy: 0.8923\n",
      "Epoch 64: val_loss did not improve from 0.20945\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.3026 - accuracy: 0.8956 - val_loss: 0.2133 - val_accuracy: 0.9192\n",
      "Epoch 65/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3055 - accuracy: 0.8956\n",
      "Epoch 65: val_loss did not improve from 0.20945\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.3055 - accuracy: 0.8956 - val_loss: 0.2178 - val_accuracy: 0.9146\n",
      "Epoch 66/100\n",
      "75/82 [==========================>...] - ETA: 0s - loss: 0.3472 - accuracy: 0.8813\n",
      "Epoch 66: val_loss did not improve from 0.20945\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.3532 - accuracy: 0.8792 - val_loss: 0.2312 - val_accuracy: 0.9116\n",
      "Epoch 67/100\n",
      "77/82 [===========================>..] - ETA: 0s - loss: 0.3375 - accuracy: 0.8843\n",
      "Epoch 67: val_loss improved from 0.20945 to 0.20193, saving model to audio_classification.hdf5\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.3300 - accuracy: 0.8864 - val_loss: 0.2019 - val_accuracy: 0.9162\n",
      "Epoch 68/100\n",
      "78/82 [===========================>..] - ETA: 0s - loss: 0.3152 - accuracy: 0.8934\n",
      "Epoch 68: val_loss did not improve from 0.20193\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.3233 - accuracy: 0.8906 - val_loss: 0.2295 - val_accuracy: 0.9101\n",
      "Epoch 69/100\n",
      "78/82 [===========================>..] - ETA: 0s - loss: 0.3244 - accuracy: 0.8878\n",
      "Epoch 69: val_loss did not improve from 0.20193\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.3237 - accuracy: 0.8883 - val_loss: 0.2352 - val_accuracy: 0.9024\n",
      "Epoch 70/100\n",
      "81/82 [============================>.] - ETA: 0s - loss: 0.3295 - accuracy: 0.8862\n",
      "Epoch 70: val_loss did not improve from 0.20193\n",
      "82/82 [==============================] - 0s 4ms/step - loss: 0.3290 - accuracy: 0.8864 - val_loss: 0.2169 - val_accuracy: 0.9101\n",
      "Epoch 71/100\n",
      "70/82 [========================>.....] - ETA: 0s - loss: 0.3253 - accuracy: 0.8830\n",
      "Epoch 71: val_loss did not improve from 0.20193\n",
      "82/82 [==============================] - 0s 4ms/step - loss: 0.3344 - accuracy: 0.8834 - val_loss: 0.2434 - val_accuracy: 0.9070\n",
      "Epoch 72/100\n",
      "81/82 [============================>.] - ETA: 0s - loss: 0.3292 - accuracy: 0.8854\n",
      "Epoch 72: val_loss did not improve from 0.20193\n",
      "82/82 [==============================] - 0s 4ms/step - loss: 0.3303 - accuracy: 0.8845 - val_loss: 0.2138 - val_accuracy: 0.9116\n",
      "Epoch 73/100\n",
      "70/82 [========================>.....] - ETA: 0s - loss: 0.3277 - accuracy: 0.8862\n",
      "Epoch 73: val_loss did not improve from 0.20193\n",
      "82/82 [==============================] - 0s 4ms/step - loss: 0.3219 - accuracy: 0.8868 - val_loss: 0.2374 - val_accuracy: 0.9177\n",
      "Epoch 74/100\n",
      "68/82 [=======================>......] - ETA: 0s - loss: 0.3083 - accuracy: 0.8925\n",
      "Epoch 74: val_loss did not improve from 0.20193\n",
      "82/82 [==============================] - 0s 4ms/step - loss: 0.3031 - accuracy: 0.8933 - val_loss: 0.2331 - val_accuracy: 0.9207\n",
      "Epoch 75/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3447 - accuracy: 0.8841\n",
      "Epoch 75: val_loss did not improve from 0.20193\n",
      "82/82 [==============================] - 0s 4ms/step - loss: 0.3447 - accuracy: 0.8841 - val_loss: 0.2534 - val_accuracy: 0.9101\n",
      "Epoch 76/100\n",
      "72/82 [=========================>....] - ETA: 0s - loss: 0.3110 - accuracy: 0.8989\n",
      "Epoch 76: val_loss did not improve from 0.20193\n",
      "82/82 [==============================] - 0s 4ms/step - loss: 0.3239 - accuracy: 0.8944 - val_loss: 0.2218 - val_accuracy: 0.9070\n",
      "Epoch 77/100\n",
      "65/82 [======================>.......] - ETA: 0s - loss: 0.3073 - accuracy: 0.8885\n",
      "Epoch 77: val_loss did not improve from 0.20193\n",
      "82/82 [==============================] - 0s 4ms/step - loss: 0.3120 - accuracy: 0.8906 - val_loss: 0.2299 - val_accuracy: 0.9177\n",
      "Epoch 78/100\n",
      "76/82 [==========================>...] - ETA: 0s - loss: 0.3254 - accuracy: 0.8865\n",
      "Epoch 78: val_loss did not improve from 0.20193\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.3226 - accuracy: 0.8868 - val_loss: 0.2068 - val_accuracy: 0.9192\n",
      "Epoch 79/100\n",
      "71/82 [========================>.....] - ETA: 0s - loss: 0.3309 - accuracy: 0.8860\n",
      "Epoch 79: val_loss did not improve from 0.20193\n",
      "82/82 [==============================] - 0s 4ms/step - loss: 0.3393 - accuracy: 0.8838 - val_loss: 0.2112 - val_accuracy: 0.9146\n",
      "Epoch 80/100\n",
      "81/82 [============================>.] - ETA: 0s - loss: 0.3046 - accuracy: 0.8900\n",
      "Epoch 80: val_loss did not improve from 0.20193\n",
      "82/82 [==============================] - 0s 4ms/step - loss: 0.3046 - accuracy: 0.8895 - val_loss: 0.2107 - val_accuracy: 0.9223\n",
      "Epoch 81/100\n",
      "74/82 [==========================>...] - ETA: 0s - loss: 0.3118 - accuracy: 0.8860\n",
      "Epoch 81: val_loss did not improve from 0.20193\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.3181 - accuracy: 0.8872 - val_loss: 0.2328 - val_accuracy: 0.9131\n",
      "Epoch 82/100\n",
      "75/82 [==========================>...] - ETA: 0s - loss: 0.3609 - accuracy: 0.8838\n",
      "Epoch 82: val_loss did not improve from 0.20193\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.3496 - accuracy: 0.8868 - val_loss: 0.2288 - val_accuracy: 0.9223\n",
      "Epoch 83/100\n",
      "79/82 [===========================>..] - ETA: 0s - loss: 0.3080 - accuracy: 0.8857\n",
      "Epoch 83: val_loss did not improve from 0.20193\n",
      "82/82 [==============================] - 0s 4ms/step - loss: 0.3088 - accuracy: 0.8845 - val_loss: 0.2173 - val_accuracy: 0.9177\n",
      "Epoch 84/100\n",
      "72/82 [=========================>....] - ETA: 0s - loss: 0.2957 - accuracy: 0.8945\n",
      "Epoch 84: val_loss did not improve from 0.20193\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.3008 - accuracy: 0.8914 - val_loss: 0.2249 - val_accuracy: 0.9223\n",
      "Epoch 85/100\n",
      "77/82 [===========================>..] - ETA: 0s - loss: 0.3078 - accuracy: 0.8989\n",
      "Epoch 85: val_loss did not improve from 0.20193\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.3036 - accuracy: 0.9002 - val_loss: 0.2626 - val_accuracy: 0.9116\n",
      "Epoch 86/100\n",
      "75/82 [==========================>...] - ETA: 0s - loss: 0.2935 - accuracy: 0.9004\n",
      "Epoch 86: val_loss did not improve from 0.20193\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.3025 - accuracy: 0.8990 - val_loss: 0.2347 - val_accuracy: 0.9253\n",
      "Epoch 87/100\n",
      "75/82 [==========================>...] - ETA: 0s - loss: 0.2797 - accuracy: 0.9075\n",
      "Epoch 87: val_loss did not improve from 0.20193\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.2742 - accuracy: 0.9089 - val_loss: 0.2336 - val_accuracy: 0.9238\n",
      "Epoch 88/100\n",
      "80/82 [============================>.] - ETA: 0s - loss: 0.3032 - accuracy: 0.8918\n",
      "Epoch 88: val_loss did not improve from 0.20193\n",
      "82/82 [==============================] - 0s 4ms/step - loss: 0.3036 - accuracy: 0.8921 - val_loss: 0.2272 - val_accuracy: 0.9268\n",
      "Epoch 89/100\n",
      "80/82 [============================>.] - ETA: 0s - loss: 0.2721 - accuracy: 0.9062\n",
      "Epoch 89: val_loss did not improve from 0.20193\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.2734 - accuracy: 0.9055 - val_loss: 0.2290 - val_accuracy: 0.9192\n",
      "Epoch 90/100\n",
      "78/82 [===========================>..] - ETA: 0s - loss: 0.3003 - accuracy: 0.8950\n",
      "Epoch 90: val_loss did not improve from 0.20193\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.2999 - accuracy: 0.8933 - val_loss: 0.2498 - val_accuracy: 0.9146\n",
      "Epoch 91/100\n",
      "76/82 [==========================>...] - ETA: 0s - loss: 0.2965 - accuracy: 0.8980\n",
      "Epoch 91: val_loss did not improve from 0.20193\n",
      "82/82 [==============================] - 0s 4ms/step - loss: 0.2951 - accuracy: 0.8982 - val_loss: 0.2123 - val_accuracy: 0.9223\n",
      "Epoch 92/100\n",
      "80/82 [============================>.] - ETA: 0s - loss: 0.2963 - accuracy: 0.8973\n",
      "Epoch 92: val_loss did not improve from 0.20193\n",
      "82/82 [==============================] - 0s 4ms/step - loss: 0.2995 - accuracy: 0.8963 - val_loss: 0.2328 - val_accuracy: 0.9207\n",
      "Epoch 93/100\n",
      "79/82 [===========================>..] - ETA: 0s - loss: 0.2877 - accuracy: 0.8975\n",
      "Epoch 93: val_loss did not improve from 0.20193\n",
      "82/82 [==============================] - 0s 4ms/step - loss: 0.2868 - accuracy: 0.8975 - val_loss: 0.2207 - val_accuracy: 0.9268\n",
      "Epoch 94/100\n",
      "74/82 [==========================>...] - ETA: 0s - loss: 0.2856 - accuracy: 0.9012\n",
      "Epoch 94: val_loss did not improve from 0.20193\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.2850 - accuracy: 0.9036 - val_loss: 0.2262 - val_accuracy: 0.9238\n",
      "Epoch 95/100\n",
      "78/82 [===========================>..] - ETA: 0s - loss: 0.3183 - accuracy: 0.8946\n",
      "Epoch 95: val_loss did not improve from 0.20193\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.3219 - accuracy: 0.8941 - val_loss: 0.2492 - val_accuracy: 0.9162\n",
      "Epoch 96/100\n",
      "74/82 [==========================>...] - ETA: 0s - loss: 0.2802 - accuracy: 0.9046\n",
      "Epoch 96: val_loss did not improve from 0.20193\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.2768 - accuracy: 0.9066 - val_loss: 0.2434 - val_accuracy: 0.9162\n",
      "Epoch 97/100\n",
      "77/82 [===========================>..] - ETA: 0s - loss: 0.2802 - accuracy: 0.9034\n",
      "Epoch 97: val_loss did not improve from 0.20193\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.2856 - accuracy: 0.9009 - val_loss: 0.2282 - val_accuracy: 0.9253\n",
      "Epoch 98/100\n",
      "75/82 [==========================>...] - ETA: 0s - loss: 0.2747 - accuracy: 0.9021\n",
      "Epoch 98: val_loss did not improve from 0.20193\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.2726 - accuracy: 0.9040 - val_loss: 0.2616 - val_accuracy: 0.9253\n",
      "Epoch 99/100\n",
      "68/82 [=======================>......] - ETA: 0s - loss: 0.2555 - accuracy: 0.9108\n",
      "Epoch 99: val_loss did not improve from 0.20193\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.2661 - accuracy: 0.9089 - val_loss: 0.2150 - val_accuracy: 0.9268\n",
      "Epoch 100/100\n",
      "80/82 [============================>.] - ETA: 0s - loss: 0.2675 - accuracy: 0.9102\n",
      "Epoch 100: val_loss did not improve from 0.20193\n",
      "82/82 [==============================] - 0s 4ms/step - loss: 0.2669 - accuracy: 0.9097 - val_loss: 0.2325 - val_accuracy: 0.9329\n",
      "0:00:41.796053\n"
     ]
    }
   ],
   "source": [
    "#Train the model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from datetime import datetime\n",
    "\n",
    "num_epochs=100\n",
    "num_batch_size=32\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='audio_classification.hdf5',verbose=1,save_best_only=True)\n",
    "start = datetime.now()\n",
    "\n",
    "model.fit(X_train,y_train,batch_size = num_batch_size,epochs=num_epochs,validation_data=(X_test,y_test),callbacks=[checkpointer])\n",
    "\n",
    "duration = datetime.now()-start\n",
    "print(duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f533918b-997e-447f-8214-324ddcbc297d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93 %\n"
     ]
    }
   ],
   "source": [
    "test_accuracy=model.evaluate(X_test,y_test,verbose=0)\n",
    "print(round(test_accuracy[1]*100),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c02c3d6e-d75c-4972-9b65-1a7e9b9793ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predictEmotion(filename):\n",
    "        audio, sample_rate = librosa.load(filename, res_type='kaiser_fast') \n",
    "        mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=182)\n",
    "        mfccs_scaled_features = np.mean(mfccs_features.T,axis=0)\n",
    "\n",
    "        #print(mfccs_scaled_features)\n",
    "        mfccs_scaled_features=mfccs_scaled_features.reshape(1,-1)\n",
    "        #print(mfccs_scaled_features)\n",
    "        #print(mfccs_scaled_features.shape)\n",
    "        predicted_label=model.predict(mfccs_scaled_features)\n",
    "        predicted_label=np.argmax(predicted_label,axis=-1)\n",
    "        print(predicted_label)\n",
    "        prediction_class = labelencoder.inverse_transform(predicted_label) \n",
    "        print(prediction_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4a53ae",
   "metadata": {},
   "source": [
    "# Prediction for samples in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "605df6fb-5699-48f7-b61a-579d1c5fdcff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\harsh\\anaconda3\\envs\\harsh_env_2\\lib\\site-packages\\keras\\engine\\training.py\", line 2041, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\harsh\\anaconda3\\envs\\harsh_env_2\\lib\\site-packages\\keras\\engine\\training.py\", line 2027, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\harsh\\anaconda3\\envs\\harsh_env_2\\lib\\site-packages\\keras\\engine\\training.py\", line 2015, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\harsh\\anaconda3\\envs\\harsh_env_2\\lib\\site-packages\\keras\\engine\\training.py\", line 1983, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\harsh\\anaconda3\\envs\\harsh_env_2\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\harsh\\anaconda3\\envs\\harsh_env_2\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_4\" is incompatible with the layer: expected shape=(None, 182), found shape=(None, 128)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[93], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpredictEmotion\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mYAF_bite_ps.wav\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[92], line 10\u001b[0m, in \u001b[0;36mpredictEmotion\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m      7\u001b[0m mfccs_scaled_features\u001b[38;5;241m=\u001b[39mmfccs_scaled_features\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#print(mfccs_scaled_features)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#print(mfccs_scaled_features.shape)\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m predicted_label\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmfccs_scaled_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m predicted_label\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39margmax(predicted_label,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(predicted_label)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\harsh_env_2\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filerz6jjki4.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\harsh\\anaconda3\\envs\\harsh_env_2\\lib\\site-packages\\keras\\engine\\training.py\", line 2041, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\harsh\\anaconda3\\envs\\harsh_env_2\\lib\\site-packages\\keras\\engine\\training.py\", line 2027, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\harsh\\anaconda3\\envs\\harsh_env_2\\lib\\site-packages\\keras\\engine\\training.py\", line 2015, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\harsh\\anaconda3\\envs\\harsh_env_2\\lib\\site-packages\\keras\\engine\\training.py\", line 1983, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\harsh\\anaconda3\\envs\\harsh_env_2\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\harsh\\anaconda3\\envs\\harsh_env_2\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_4\" is incompatible with the layer: expected shape=(None, 182), found shape=(None, 128)\n"
     ]
    }
   ],
   "source": [
    "predictEmotion('YAF_bite_ps.wav')# surpise file (female)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ef95fc32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 43ms/step\n",
      "[3]\n",
      "['happy']\n"
     ]
    }
   ],
   "source": [
    "predictEmotion('DC_a01.wav')#angry file (Male)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e8db5a27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 49ms/step\n",
      "[1]\n",
      "['disgust']\n"
     ]
    }
   ],
   "source": [
    "predictEmotion('OAF_cab_sad.wav')#sad file (female)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "38d188f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 35ms/step\n",
      "[3]\n",
      "['happy']\n"
     ]
    }
   ],
   "source": [
    "predictEmotion('YAF_rose_happy.wav')#happy file (female)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5b75ee2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 30ms/step\n",
      "[4]\n",
      "['neutral']\n"
     ]
    }
   ],
   "source": [
    "predictEmotion('DC_n24.wav')#neutral (Male)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4a421c",
   "metadata": {},
   "source": [
    "# Prediction for samples not in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7e4ee4ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 34ms/step\n",
      "[3]\n",
      "['happy']\n"
     ]
    }
   ],
   "source": [
    "predictEmotion('03-01-08-01-01-01-24.wav')# surprised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f5285112",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 48ms/step\n",
      "[3]\n",
      "['happy']\n"
     ]
    }
   ],
   "source": [
    "predictEmotion('03-01-08-01-01-01-24.wav')# fearful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1182a46c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 40ms/step\n",
      "[1]\n",
      "['disgust']\n"
     ]
    }
   ],
   "source": [
    "predictEmotion('03-01-01-01-01-01-10.wav')#neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "60063ad8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 45ms/step\n",
      "[3]\n",
      "['happy']\n"
     ]
    }
   ],
   "source": [
    "predictEmotion('03-01-05-01-01-01-05.wav')#angry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "28afae1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 46ms/step\n",
      "[1]\n",
      "['disgust']\n"
     ]
    }
   ],
   "source": [
    "predictEmotion('03-01-03-01-01-01-17.wav')#happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38586754",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
